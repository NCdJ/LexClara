{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f350e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Pattern\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate   \n",
    ")\n",
    "from langchain_huggingface import (\n",
    "    ChatHuggingFace\n",
    "    , HuggingFaceEmbeddings\n",
    "    , HuggingFaceEndpoint\n",
    ")\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "transformers_logging.set_verbosity_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d4888",
   "metadata": {},
   "source": [
    "**Definição dos locais onde se encontram a base de dados vetorial, modelos de embeddings e LLM e ficheiro com o resultado das avaçiações ao modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_dir = \"lexclaraDB/ChromaDB\"\n",
    "model_llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "embeddings_name = \"BAAI/bge-m3\"\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b19535",
   "metadata": {},
   "source": [
    "**Inicialização do modelo de embeddings local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:54:43,949 - INFO - Load pretrained SentenceTransformer: /nelson/LexClara/models/huggingface/BAAI-bge-m3\n",
      "loading configuration file /nelson/LexClara/models/huggingface/BAAI-bge-m3/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 8194,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file /nelson/LexClara/models/huggingface/BAAI-bge-m3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at /nelson/LexClara/models/huggingface/BAAI-bge-m3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# documentar as definições do modelo de embeddings\n",
    "model_kwargs = {'device': 'cuda'\n",
    "                , 'trust_remote_code': True\n",
    "                }\n",
    "\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66dc2d",
   "metadata": {},
   "source": [
    "**Inicialização da base de dados vetorial - ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:55:45,126 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-10-29 19:55:45,139 - INFO - Coleção 'LexClara_bge_m3_1024' existente carregada.\n"
     ]
    }
   ],
   "source": [
    "# Inicializar cliente persistente\n",
    "client = chromadb.PersistentClient(path=chroma_db_dir)\n",
    "\n",
    "# Nome da coleção\n",
    "collection_name = \"LexClara_bge_m3_1024\" # documentos breves\n",
    "\n",
    "colecao = client.get_collection(name=collection_name)\n",
    "\n",
    "# Conectar à coleção com LangChain\n",
    "vectordb = Chroma(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=hf_embeddings,\n",
    "    persist_directory=chroma_db_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c0085",
   "metadata": {},
   "source": [
    "**Início das funções auxiliares para o LExClaraBot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114145c",
   "metadata": {},
   "source": [
    "**Função que permite reconhecer padrões no texto colocado na pergunta ao LLM**\n",
    "\n",
    "Neste momento só permite o reconhecimento de dois tipos de diploma &mdash; Decreto-Lei e Decreto Regulamentar &mdash; correspondentes aos diplomas que possuem linguagem clara, extraídos do DRE-Tretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db86ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PADROES = [\n",
    "    re.compile(r'(Decreto[-\\s]?Lei)\\s*(?:n\\.º\\s*)?(\\d+(?:[A-Za-z-]*/\\d{4}))', re.IGNORECASE),\n",
    "    # re.compile(r'(Lei)\\s*(?:n\\.º\\s*)?(\\d+(?:[A-Za-z-]*/\\d{4}))', re.IGNORECASE),\n",
    "    re.compile(r'(Decreto\\s+Regulamentar)\\s*(?:n\\.º\\s*)?(\\d+(?:[A-Za-z-]*/\\d{4}))', re.IGNORECASE),\n",
    "    # re.compile(r'(Portaria)\\s*(?:n\\.º\\s*)?(\\d+(?:[A-Za-z-]*/\\d{4}))', re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extrair_por_regex(pergunta: str, patterns: List[Pattern]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrai do texto todos os identificadores completos (tipo + número),\n",
    "    sem o 'n.º'. Exemplo de saída: ['Decreto-Lei 137/2023', 'Lei 12/2022'].\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    for pat in patterns:\n",
    "        for match in pat.finditer(pergunta):\n",
    "            tipo = match.group(1).strip()\n",
    "            numero = match.group(2).strip()\n",
    "            resultados.append(f\"{tipo} {numero}\")\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c85c62",
   "metadata": {},
   "source": [
    "**Carregar o dataset de linguagem clara_2 para obter um diploma aleatório para testar as respostas do LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e056c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:56:01,877 - INFO - Dados do ficheiro /nelson/LexClara/data/gold/linguagem-clara-2020-2024_2.csv foram carregados com sucesso.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_linguagem_clara_2 = pd.read_csv(filepath_or_buffer=os.path.join(os.getcwd(),'data','gold','linguagem-clara-2020-2024_2.csv'))\n",
    "except Exception as e:\n",
    "        print(f\"Ocorreu o seguinte erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484cfbf",
   "metadata": {},
   "source": [
    "**Função para escolher um exemplo aleatório**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a90d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gera_exemplo_aleatorio_do_df(df):\n",
    "\n",
    "    tuplo_per_res=[]\n",
    "\n",
    "    linha = df.sample(n=1).iloc[0]\n",
    "\n",
    "    perguntas=[\n",
    "            f\"O que é o diploma {linha['identificacao_diploma']}?\",\n",
    "            f\"O que vai mudar com o diploma {linha['identificacao_diploma']}?\",\n",
    "            f\"Que vantagens traz o diploma {linha['identificacao_diploma']}?\",\n",
    "            f\"Quando entra em vigor o diploma {linha['identificacao_diploma']}?\"\n",
    "        ]\n",
    "\n",
    "    for i, perg in enumerate(perguntas):\n",
    "\n",
    "        if i==0:\n",
    "            res=linha['o_que_e']\n",
    "\n",
    "        if i==1:\n",
    "            res=linha['o_que_vai_mudar']\n",
    "\n",
    "        if i==2:\n",
    "            res=linha['que_vantagens_traz']\n",
    "            \n",
    "        if i==3:\n",
    "            res=linha['quando_entra_em_vigor']\n",
    "\n",
    "        tuplo_per_res.append((perg, res)) #type:ignore\n",
    "\n",
    "    return tuplo_per_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064cacf2",
   "metadata": {},
   "source": [
    "**Permite usar o exemplo aleatório no interface criado no Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d9e3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gera_exemplo_aleatorio_para_gradio():\n",
    "\n",
    "    pergunta, resposta = random.choice(gera_exemplo_aleatorio_do_df(df=df_linguagem_clara_2))\n",
    "    \n",
    "    return pergunta, resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641e058",
   "metadata": {},
   "source": [
    "**Definição do _prompt_ de forma a ajudar o LLM a responder melhor às perguntas colocadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed108db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construir_prompt_few_shot() -> ChatPromptTemplate:\n",
    "\n",
    "\n",
    "    # 1. Definição das mensagens de sistema e de utilizador\n",
    "    sys_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"És um chatbot de assistência jurídica que ajuda o utilizador leigo a ter contacto com as leis portuguesas.\\\n",
    "        Responde à pergunta com base no contexto legislativo existente na base de dados. \\\n",
    "        Escreve frases completas com escrita e pontuação corretas. \\\n",
    "        Se o contexto não contiver informação suficiente, responde exatamente:  \\\n",
    "        Não há informação relevante nos diplomas selecionados. \\\n",
    "        Não inventes respostas que não estejam no contexto. \\\n",
    "        Os exemplos de diálogo apresentados servem apenas para demonstrar o formato e o estilo da resposta esperada, não devem ser repetidos. \\\n",
    "        \")\n",
    "\n",
    "\n",
    "    exemplos = gera_exemplo_aleatorio_do_df(df=df_linguagem_clara_2)\n",
    "\n",
    "    mensagens_exemplo = []\n",
    "\n",
    "    for entrada, resposta in exemplos:\n",
    "        mensagens_exemplo.append(HumanMessagePromptTemplate.from_template(entrada))\n",
    "        mensagens_exemplo.append(AIMessagePromptTemplate.from_template(resposta))\n",
    "\n",
    "    input_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"Contexto legislativo relevante:\\n{context}\\n\\n \\\n",
    "        Pergunta:\\n{input}\\n\\n \\\n",
    "        Resposta\")\n",
    "\n",
    "    return ChatPromptTemplate.from_messages(\n",
    "        [sys_message] + mensagens_exemplo + [input_prompt]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625af49",
   "metadata": {},
   "source": [
    "**Criação das definições do LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_llm(\n",
    "    temperature: float,\n",
    "    top_k: int,\n",
    "    top_p: float,\n",
    "    max_tokens: int,\n",
    "    repetition_penalty: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Cria e retorna um LLM da Mistral com os parâmetros ajustáveis.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "\n",
    "    generator = HuggingFaceEndpoint(\n",
    "        repo_id=repo_id\n",
    "        ,task=\"text-generation\"\n",
    "        ,temperature=temperature\n",
    "        ,top_k=top_k\n",
    "        ,top_p=top_p\n",
    "        ,max_new_tokens=max_tokens\n",
    "        ,repetition_penalty=repetition_penalty\n",
    "        ,huggingfacehub_api_token=hf_token\n",
    "        ,do_sample=True\n",
    "        ,streaming=True\n",
    "        ,return_full_text=False\n",
    "    )\n",
    "\n",
    "    llm = ChatHuggingFace(llm=generator)\n",
    "    \n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b180c6",
   "metadata": {},
   "source": [
    "**Função auxiliar que permite a recolha do contexto e lidar com casos sem documentos retornados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acacb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responder_pelo_gradio_com_LLM(\n",
    "    pergunta: str,\n",
    "    temperature: float,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    max_tokens: int,\n",
    "    repetition_penalty: float,\n",
    "):\n",
    "\n",
    "    # Extrai os identificadores de diploma da pergunta, para serem utilizado como critério de filtragem de documentos\n",
    "\n",
    "    ids = extrair_por_regex(pergunta, PADROES)\n",
    "\n",
    "    if not ids:\n",
    "        return \"Nenhum identificador de diploma encontrado na pergunta.\"\n",
    "        exit\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\"\n",
    "                                    , search_kwargs={\n",
    "                                        'filter': {\n",
    "                                            'diploma': {'$in': ids}  # O mesmo filtro de metadados\n",
    "                                        },\n",
    "                                        'k': top_k  # O mesmo número de documentos a serem retornados\n",
    "                                    }\n",
    "                                )\n",
    "    \n",
    "    # Recupera documentos\n",
    "    retrieved_docs = retriever.invoke(pergunta)\n",
    "    if not retrieved_docs:\n",
    "        contexto = \"Nenhum contexto legislativo relevante foi encontrado para os diplomas mencionados.\"\n",
    "        chunks = \"\"\n",
    "    else:\n",
    "        contexto = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        chunks = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Prepara o LLM com os parâmetros recebidos, e presentes no interface\n",
    "    llm = criar_llm(temperature\n",
    "                    , top_k\n",
    "                    , top_p\n",
    "                    , max_tokens\n",
    "                    , repetition_penalty)\n",
    "    \n",
    "    # construção do prompt de auxílio ao LLM para as respostas\n",
    "    prompt = construir_prompt_few_shot()\n",
    "\n",
    "\n",
    "    # Cria a chain de combinação de documentos (stuff)\n",
    "    combine_docs_chain = create_stuff_documents_chain(\n",
    "        llm=llm\n",
    "        ,prompt=prompt\n",
    "    )\n",
    "\n",
    "    chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=combine_docs_chain\n",
    "    )\n",
    "\n",
    "    result = chain.invoke({\n",
    "        \"context\": contexto,\n",
    "        \"input\": pergunta})\n",
    "\n",
    "\n",
    "        \n",
    "    # A query_dict virá da chain e conterá 'pergunta'\n",
    "    retrieved_docs = retriever.invoke(pergunta)\n",
    "    \n",
    "    chunks = (\n",
    "        \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "        if retrieved_docs\n",
    "        else \"Não foram encontrados documentos que correspondem ao critério.\"\n",
    "    )\n",
    "\n",
    "    resposta = result.get(\"answer\") or result.get(\"output_text\") or str(result)\n",
    "\n",
    "    return resposta, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae50b4",
   "metadata": {},
   "source": [
    "**Geração do interface com utilizado, via Gradio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13849e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:56:58,791 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with gr.Blocks(title=\"Pergunte à Legislação com Mistral\", theme=gr.themes.Default(text_size=\"lg\")) as chatbot_LexClara: # type: ignore\n",
    "\n",
    "\n",
    "    gr.Markdown(\"##Chat Jurídico com Mistral (Few-shot + Parametrização)\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        pergunta_input = gr.Textbox(label=\"Pergunta\"\n",
    "                                    , lines = 2\n",
    "                                    , placeholder=\"Ex: O que é o Decreto-Lei n.º 137/2023?\")\n",
    "        \n",
    "        usar_exemplo_btn = gr.Button(\"Usar Exemplo Aleatório\")\n",
    "    \n",
    "    with gr.Accordion(\"Parâmetros Avançados\", open=False):\n",
    "        temperature = gr.Slider(minimum=0\n",
    "                                , maximum=1\n",
    "                                , value=0.7\n",
    "                                , step=0.1\n",
    "                                , label=\"Temperatura\")\n",
    "        \n",
    "        top_p = gr.Slider(minimum=0\n",
    "                          , maximum=1\n",
    "                          , value=1.0\n",
    "                          , step=0.05\n",
    "                          , label=\"Top-p\")\n",
    "        \n",
    "        top_k = gr.Slider(minimum=1\n",
    "                          , maximum=50\n",
    "                          , value=5\n",
    "                          , step=1\n",
    "                          , label=\"Top-k\")\n",
    "        \n",
    "        max_tokens = gr.Slider(minimum=100\n",
    "                               , maximum=2000\n",
    "                               , value=512\n",
    "                               , step=100\n",
    "                               , label=\"Número máximo de tokens gerados\")\n",
    "        \n",
    "        repetition_penalty = gr.Slider(minimum=1.0\n",
    "                                       , maximum=2.0\n",
    "                                       , value=1.2\n",
    "                                       , step=0.1\n",
    "                                       , label=\"Penalização por repetição\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        resposta_output = gr.Textbox(label=\"Resposta do LLM\", lines=4)\n",
    "        chunks_output = gr.Textbox(label=\"Segmentos de Texto Recuperados\", lines=8)\n",
    "\n",
    "    resposta_esperada_output = gr.Textbox(label=\"Resposta Esperada (para avaliação)\", lines=4)\n",
    "\n",
    "    perguntar_btn = gr.Button(\"Obter Resposta\")\n",
    "\n",
    "    # Funções aplicadas aos botões\n",
    "    usar_exemplo_btn.click(\n",
    "        gera_exemplo_aleatorio_para_gradio,\n",
    "        inputs=[],\n",
    "        outputs=[pergunta_input\n",
    "                , resposta_esperada_output]\n",
    "    )\n",
    "\n",
    "    perguntar_btn.click(\n",
    "        responder_pelo_gradio_com_LLM,\n",
    "        inputs=[pergunta_input\n",
    "                , temperature\n",
    "                , top_p\n",
    "                , top_k\n",
    "                , max_tokens\n",
    "                , repetition_penalty],\n",
    "        outputs=[resposta_output\n",
    "                , chunks_output]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "595c14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:57:03,049 - INFO - HTTP Request: GET http://localhost:8085/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-10-29 19:57:03,062 - INFO - HTTP Request: HEAD http://localhost:8085/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:57:03,796 - INFO - HTTP Request: GET https://api.gradio.app/v3/tunnel-request \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://0c1863bf4a5f81333e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 19:57:05,867 - INFO - HTTP Request: HEAD https://0c1863bf4a5f81333e.gradio.live \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0c1863bf4a5f81333e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 0.0.0.0:8085 <> https://0c1863bf4a5f81333e.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_LexClara.launch(server_name=\"0.0.0.0\"\n",
    "                        , server_port=8085\n",
    "                        , share=True\n",
    "                        , debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
